---
title: 500行或更少系列之爬虫
date: 2018-01-08 15:22:18
tags:
- python
categories:
- python
description: 用500行左右的代码实现一个爬虫
---

*原文请见：http://aosabook.org/en/500L/a-web-crawler-with-asyncio-coroutines.html*
*代码请见：https://github.com/aosabook/500lines/tree/master/crawler*

# 引言
经典的计算机科学强调的是高效的算法，能够尽可能快地完成计算。但是许多联网的程序并没有耗时在计算上面，而是打开许多慢的连接，或者是不常发生的事件。这些程序提出了一个不同的挑战：有效地等待大量的网络事件。针对这个问题的一种现代方法是异步 I/O 或 async。

本章介绍了一个简单的网络爬虫。爬虫是一个天生异步应用程序，因为它等待许多响应，但很少进行计算。它能同时读取的页面越多，它完成的时间就越快。如果它为每个请求分配一个线程，那么当并发请求的数量上升时，它会在耗尽套接字之前先耗尽内存或其他与线程相关的资源。可以使用异步 I/O 来避免使用线程。

我们分三个阶段展示这个例子。首先，我们展示了一个异步事件循环，并创建了一个使用回调的事件循环的爬行器：它非常高效，但是将它扩展到更复杂的问题会导致无法维护的面条代码。第二，因此，我们展示了 Python 的协同程序是有效的和可扩展的。我们在 Python 中使用生成器函数实现简单的代码。在第三阶段，我们使用 Python 的标准 asyncio 库中的功能完整的 coroutines，并使用异步队列进行协调。


# 任务
一个网络爬虫在一个网站上找到并下载所有的页面，也许是为了存档或索引它们。从根 URL 开始，它获取每个页面，解析出未访问的链接，并将这些链接添加到队列中。当它获取到一个没有未访问链接的页面且队列是空的时，爬虫停止工作。
我们可以通过同时下载多个页面来加快这一进程。当爬虫找到新的链接时，它会在单独的套接字上启动对新页面的同步抓取操作。当它们返回时，它会解析响应，并添加新链接到队列。由于过多的并发会降低性能，所以可能会出现一些收益递减的问题，因此我们限制了并发请求的数量，并将剩余的链接留在队列中，直到完成了一些请求后继续处理。


# 传统的方法
我们如何使爬虫并行运行？传统上，我们会创建一个线程池。每个线程将负责在套接字上一次下载一个页面。例如，从 xkcd.com 下载一个页面：
```python
def fetch(url):
    sock = socket.socket()
    sock.connect(('xkcd.com', 80))
    request = 'GET {} HTTP/1.0\r\nHost: xkcd.com\r\n\r\n'.format(url)
    sock.send(request.encode('ascii'))
    response = b''
    chunk = sock.recv(4096)
    while chunk:
        response += chunk
        chunk = sock.recv(4096)

    # Page is now downloaded.
    links = parse_links(response)
    q.add(links)
```

默认情况下，套接字操作是阻塞的。因此，为了同时下载多个页面，我们需要很多线程。
然而，线程是昂贵的，操作系统对进程、用户或机器可能拥有的线程数量执行各种严格的限制。对于非常大规模的应用程序，有成千上万的连接，该如何处理这种情况呢？

# Async
异步 I/O 框架使用非阻塞套接字在单个线程上执行并发操作。在我们的 async 爬虫中，我们在开始连接服务器之前设置了套接字非阻塞：

```python
sock = socket.socket()
sock.setblocking(False)
try:
    sock.connect(('xkcd.com', 80))
except BlockingIOError:
    pass
```

令人恼火的是，一个非阻塞的套接字会抛出一个连接的异常，即使它正常工作。这个异常复制了底层 C 函数的令人恼火的行为，它将 errno 设置为 EINPROGRESS 来告诉您它已经开始了。
现在我们的爬虫需要一种方法来知道什么时候建立连接，所以它可以发送 HTTP 请求。我们可以简单地循环下去:

```python
request = 'GET {} HTTP/1.0\r\nHost: xkcd.com\r\n\r\n'.format(url)
encoded = request.encode('ascii')

while True:
    try:
        sock.send(encoded)
        break  # Done.
    except OSError as e:
        pass

print('sent')
```
这种方法不仅浪费电力，而且不能有效地等待多个 socket 上的事件。最早，BSD Unix 对这个问题的解决方案是 select。如今，对互联网应用需要处理大量的连接，这导致了类似于 poll 的替代者，然后是在 BSD 上的 kqueue 和 Linux 上的 epoll。这些 api 类似于select，但是在大量的连接上工作得很好。
这些方法有个专用的名词，叫做多路复用，更多请见[IO 多路复用是什么意思？](https://www.zhihu.com/question/32163005)
Python 3.4 的 `DefaultSelector` 使用了系统上可用的最佳函数。可以像下面这样来注册 I/O 事件：

```python
from selectors import DefaultSelector, EVENT_WRITE

selector = DefaultSelector()

sock = socket.socket()
sock.setblocking(False)
try:
    sock.connect(('xkcd.com', 80))
except BlockingIOError:
    pass

def connected():
    selector.unregister(sock.fileno())
    print('connected!')

selector.register(sock.fileno(), EVENT_WRITE, connected)
```

我们忽略了假的错误（上文已说明）并调用了 `selector.register`。传入套接字的文件描述符和一个常量来表示我们正在等待的事件。要在建立连接时被通知，我们传递 EVENT_WRITE：也就是说，我们想知道套接字是何时“可写”的。我们还传递了一个 Python 函数，连接到该事件发生时运行。这样的函数称为回调函数。
我们在一个循环中处理 I/O 通知，当选择器接收到数据时：

```python
def loop():
    while True:
        events = selector.select()
        for event_key, event_mask in events:
            callback = event_key.data
            callback()
```

# 基于回调函数编程
在我们已经构建的 async 框架中，如何构建网络爬虫？即使是一个简单的 url fetcher 也很痛苦。
我们从尚未取回的 url 的全局集合开始，以及我们已经爬取过的 url 集合：

```python
urls_todo = set(['/'])
seen_urls = set(['/'])
```

获取一个页面需要一系列的回调。当套接字连接时，连接的回调触发，并向服务器发送 GET 请求。但它必须等待响应，因此它注册另一个回调。如果当回调触发时，它无法读取完整的响应，它会再次注册，等等。
让我们将这些回调收集到一个 Fetcher 对象中。它需要一个 url、一个 host、 一个套接字对象和一个存放响应字节的地方:

```python
class Fetcher:
    def __init__(self, url, host):
        self.response = b''
        self.url = url
        self.host = host
        self.sock = None
```

通过调用 `Fetcher.fetch` 开始爬取：

```python
def fetch(self):
    global concurrency_achieved
    concurrency_achieved = max(concurrency_achieved, len(urls_todo))

    self.sock = socket.socket()
    self.sock.setblocking(False)
    try:
        self.sock.connect((self.host, 80))
    except BlockingIOError:
        pass
    selector.register(self.sock.fileno(), EVENT_WRITE, self.connected)
```

`fetch` 方法会创建一个 socket 并连接远程的 socket。但是注意，在建立连接之前，方法会返回。它必须返回对事件循环的控制以等待连接。为了理解其中的原因，想象一下我们整个应用程序的结构：

```python
fetcher = Fetcher('/', 'extremevision.com.cn')
fetcher.fetch()

while not stopped:
    events = selector.select()
    for event_key, event_mask in events:
        callback = event_key.data
        callback(event_key, event_mask)
```

所有事件通知都在事件循环中被处理，当调用 select 时。因此，fetch 必须将控制权交给事件循环，以便程序知道套接字是何时连接的。只有这样，循环才能运行连接的回调，该回调是在上面的 fetch 结束时注册的。
这里是连接的实现：

```python
def connected(self, key, mask):
    selector.unregister(key.fd)
    get = 'GET {} HTTP/1.0\r\nHost: {}\r\n\r\n'.format(self.url, self.host)
    self.sock.send(get.encode('ascii'))
    selector.register(key.fd, EVENT_READ, self.read_response)
```

该方法发送一个 GET 请求。实际应用程序将检查 `send` 的返回值，以防整个消息不能同时发送。但我们的请求很小，可以暂不考虑。然后等待响应，当然，它必须注册另一个回调，并将控制权移交给下一个和最后一个回调事件 read_response，处理服务器的回复：

```python
def read_response(self, key, mask):
    global stopped

    chunk = self.sock.recv(4096)  # 4k chunk size.
    if chunk:
        self.response += chunk
    else:
        selector.unregister(key.fd)  # Done reading.
        links = self.parse_links()
        # print(self.response.decode('utf-8'))
        for link in links.difference(seen_urls):
            urls_todo.add(link)
            Fetcher(link, self.host).fetch()

        seen_urls.update(links)
        urls_todo.remove(self.url)
        if not urls_todo:
            stopped = True
        print(self.url)
```

每次选择器看到套接字是“可读的”时，都会执行回调，这意味着两个东西：套接字有数据或关闭。
回调请求从套接字中获取多达 4kb 的数据。如果数据少于 4kb，块将全部接收这些数据。如果数据多于 4kb，块将包含 4kb 的数据 ，而套接字仍然是可读的，因此事件循环在下一个 tick 上再次运行这个回调。当响应完成时，服务器关闭套接字，而 chunk 将是空的。
parse_links 方法(未显示)返回一组 url。我们为每个新 url 启动一个新的 fetcher，并没有并发性限制。基于回调异步编程有这样一个特性：我们不需要对共享数据进行任何的互斥操作，比如当我们添加 url 到 seen_urls 时。
我们添加一个全局 `stopped` 变量，并使用它来控制循环：

```python
while not stopped:
    events = selector.select()
    for event_key, event_mask in events:
        callback = event_key.data
        callback(event_key, event_mask)
```


# 协程
可以通过一种称为“协同”的模式，将回调的效率与多线程编程的方式结合起来。使用 Python 3.4 的标准 asyncio 库和一个名为 aiohttp 的包，在 coroutine 中获取一个 url 是非常直接的：

```python
@asyncio.coroutine
def fetch(self, url):
    response = yield from self.session.get(url)
    body = yield from response.read()
```

它也是可扩展的。与每个线程的 50k 内存和操作系统对线程的硬限制相比，Python 的 coroutine 只占用 Jesse 系统的 3k 内存。Python 可以轻松地启动成千上万的coroutines。
coroutine 的概念，可以追溯到计算机科学时代，很简单：它是一个可以暂停和恢复的子程序。coroutines 可以让多任务合作地处理：它们选择什么时候暂停，然后选择哪个 coroutine 运行。
coroutines 的实现有很多；甚至在 Python 中也有几个。Python 3.4 中标准的“asyncio”库中的 coroutines 是建立在 generators、Future class 和 “yield from” 语句之上的。从 Python 3.5 开始，coroutines 是该语言的原生特性；然而，理解 coroutines 是在 Python 3.4 中首次实现的，使用早就存在的语言特性，是解决 Python 3.5 的原生协同程序的基础。
为了解释 Python 3.4 的基于生成器的 coroutines，我们将阐述生成器，以及它们如何在 asyncio 中用作 coroutines，并且相信您会喜欢阅读它，就像我们喜欢编写它一样。一旦我们解释了基于生成器的协同程序，我们将在我们的 async 网络爬虫中使用它们。

# Python 生成器是如何工作的
在您掌握 Python 生成器之前，您必须了解 Python 函数的正常工作原理。通常，当 Python 函数调用子函数时，子函数将保持控制，直到它返回或抛出异常。然后控制返回给调用者:

```python
>>> def foo():
...     bar()
...
>>> def bar():
...     pass
```

标准的 Python 解释器是用 C 语言编写的，执行 Python 函数的 C 函数被称为 PyEval_EvalFrameEx。它接受一个 Python 堆栈帧对象，并在帧的上下文中对 Python 字节码进行求值。这是 foo 的字节码:

```python
>>> import dis
>>> dis.dis(foo)
  2           0 LOAD_GLOBAL              0 (bar)
              3 CALL_FUNCTION            0 (0 positional, 0 keyword pair)
              6 POP_TOP
              7 LOAD_CONST               0 (None)
             10 RETURN_VALUE
```

foo 函数将 bar 加载到它的堆栈上并调用它，然后从堆栈中弹出它的返回值，将 None 加载到堆栈上，然后返回 None。
当 PyEval_EvalFrameEx 遇到 CALL_FUNCTION 字节码时，它会创建一个新的 Python 堆栈帧和递归：也就是说，它用新的帧递归调用 PyEval_EvalFrameEx，该帧用于执行 bar。
理解 Python 堆栈帧在堆内存中分配至关重要! Python 解释器是一个普通的 C 程序，因此它的堆栈帧是正常的堆栈帧。但是它操纵的 Python 堆栈帧在堆上。这意味着 Python 堆栈帧可以比它的函数调用更有效。为了演示，请在 bar 中保存当前帧：

```python
>>> import inspect
>>> frame = None
>>> def foo():
...     bar()
...
>>> def bar():
...     global frame
...     frame = inspect.currentframe()
...
>>> foo()
>>> # The frame was executing the code for 'bar'.
>>> frame.f_code.co_name
'bar'
>>> # Its back pointer refers to the frame for 'foo'.
>>> caller_frame = frame.f_back
>>> caller_frame.f_code.co_name
'foo'
```

![](function-calls.png)

现在轮到 Python 生成器出场了，它使用相同的构建块——代码对象和堆栈帧——来达到不可思议的效果。

这是一个生成器：

```python
>>> def gen_fn():
...     result = yield 1
...     print('result of yield: {}'.format(result))
...     result2 = yield 2
...     print('result of 2nd yield: {}'.format(result2))
...     return 'done'
...     
```

当 Python 编译 gen_fn 到字节码时，它会看到 yield 语句，并且知道 gen_fn 是一个生成器函数，而不是普通的函数。它设置了一个标志来记住这个事实：

```python
>>> # The generator flag is bit position 5.
>>> generator_bit = 1 << 5
>>> bool(gen_fn.__code__.co_flags & generator_bit)
True
```

当你调用生成器函数时，Python 会看到生成器标志，而它实际上并没有运行该函数。而是创建了一个生成器：

```python
>>> gen = gen_fn()
>>> type(gen)
<class 'generator'>
```

Python 生成器封装了一个堆栈帧，并引用了一些代码，gen_fn 的主体:

```python
>>> gen.gi_code.co_name
'gen_fn'
```

所有调用 gen_fn 生成的生成器指向相同的代码。但是每个都有自己的堆栈帧。这个堆栈帧不在于任何实际的堆栈上，它位于堆内存中等待被使用：

![](generator.png)

堆栈帧有个 “last instruction” 指针，指向最近执行的那条指令。刚开始的时候 last instruction 指针是 -1，意味着生成器尚未开始：

```python
>>> gen.gi_frame.f_lasti
-1
```

当我们调用 send 时，生成器达到第一个 yield 处然后暂停执行。send 的返回值是 1，这是因为 gen 把 1 传给了 yield 表达式：

```python
>>> gen.send(None)
1
```

现在生成器的指令指针（instruction pointer）向前移动了 3 个字节码，这些是编译好的 56 字节的 Python 代码的一部分：

```python
>>> gen.gi_frame.f_lasti
3
>>> len(gen.gi_code.co_code)
56
```
生成器可以在任何时候被任何函数恢复执行，因为它的堆栈帧实际上不在堆栈上——它在堆（内存）上。生成器在调用层次结构中的位置不是固定的，它不需要遵循常规函数执行时遵循的先进后出顺序。生成器是被解放了的，它像云一样浮动。

我们可以将 “hello” 发送到这个生成器中，它会成为 yield 表达式的值，然后生成器会继续执行，直到产出 yield 出 2：

```python
>>> gen.send('hello')
result of yield: hello
2
```

现在这个生成器的堆栈帧包含局部变量 result：

```python
>>> gen.gi_frame.f_locals
{'result': 'hello'}
```

从 gen_fn 创建的其他生成器将具有自己的堆栈帧和局部变量。

当我们再次调用 send 时，生成器将从它第二个 yield 处继续执行，然后以产生特殊异常 StopIteration 结束：

```python
>>> gen.send('goodbye')
result of 2nd yield: goodbye
Traceback (most recent call last):
    File "<input>", line 1, in <module>
StopIteration: done
```

异常有一个值，它是那个生成器的返回值：字符串 “done”。

# 使用生成器构建协程
因此，生成器可以暂停，并且可以以一个值恢复，并且它具有一个返回值。这听起来是构建一个异步编程模型的原始版本，而不需要使用意大利面式的回调！我们想要构建一个 “coroutine”：一个与程序中其他 routines 协同调度的 routines。我们的 coroutines 将是 Python 标准的 “asyncio” 库的简化版本。在 asyncio 中，我们将使用 generators, futures, 和 "yield from"。
首先，我们需要一种方法来表示一个协同程序等待的未来的结果。一个简化的实现如下：

```python
class Future:
    def __init__(self):
        self.result = None
        self._callbacks = []

    def add_done_callback(self, fn):
        self._callbacks.append(fn)

    def set_result(self, result):
        self.result = result
        for fn in self._callbacks:
            fn(self)
```

一个 future 对象一开始是 “pending” 状态。它通过调用 set_result 来 “resolve”。
让我们调整我们的 fetcher 使用 Future 和 coroutines：

```python
class Fetcher:
    def fetch(self):
        self.sock = socket.socket()
        self.sock.setblocking(False)
        try:
            self.sock.connect(('xkcd.com', 80))
        except BlockingIOError:
            pass
        selector.register(self.sock.fileno(),
                          EVENT_WRITE,
                          self.connected)

    def connected(self, key, mask):
        print('connected!')
        # And so on....
```

fetch 方法开始连接一个套接字，然后注册回调 `connected`，在套接字就绪时执行。现在我们可以把这两个步骤合并成一个 coroutine：

```python
def fetch(self):
    sock = socket.socket()
    sock.setblocking(False)
    try:
        sock.connect(('xkcd.com', 80))
    except BlockingIOError:
        pass

    f = Future()

    def on_connected():
        f.set_result(None)

    selector.register(sock.fileno(),
                      EVENT_WRITE,
                      on_connected)
    yield f
    selector.unregister(sock.fileno())
    print('connected!')
```

现在 fetch 是一个生成器函数，而不是普通的函数，因为它包含一个 yield 语句。我们创建一个挂起的 future 对象，然后让它在套接字准备好之前暂停取回。内部函数 `on_connected` 会 resolve。

但当 future resolve 的时候，是什么恢复了生成器？我们需要一个 coroutine driver。让我们称之为 “task”：

```python
class Task:
    def __init__(self, coro):
        self.coro = coro
        f = Future()
        f.set_result(None)
        self.step(f)

    def step(self, future):
        try:
            next_future = self.coro.send(future.result)
        except StopIteration:
            return

        next_future.add_done_callback(self.step) # 执行f.set_result(None)时会调用这里的self.step

# Begin fetching http://xkcd.com/353/
fetcher = Fetcher('/353/')
Task(fetcher.fetch())

loop()
```

task 启动 fetch 生成器，并将 None 传递给 send 方法，然后取回运行，直到它产生一个 next_future。当连接到套接字时，事件循环将运行回调 on_connected，调用 set_result，最终会调用 step。

# 使用 yield from 重构 Coroutines
关于 yield from 可以参考《流畅的python》第 16 章或者下图
![](yield-from.jpg)

一旦连接了套接字，我们就发送 HTTP GET 请求并读取服务器响应。这些步骤不再需要分散在回调中；我们把它们收集到同一个生成器中：

```python
def fetch(self):
    # ... connection logic from above, then:
    sock.send(request.encode('ascii'))

    while True:
        f = Future()

        def on_readable():
            f.set_result(sock.recv(4096))

        selector.register(sock.fileno(),
                          EVENT_READ,
                          on_readable)
        chunk = yield f
        selector.unregister(sock.fileno())
        if chunk:
            self.response += chunk
        else:
            # Done reading.
            break
```
这段代码从一个套接字中读取整个消息，似乎很有用。我们如何将它分解为一个 subroutine ？轮到 `yield from` 登场了。它让一个生成器委托给另一个生成器。
为了清楚它是怎么工作的，让我们回到简单的生成器示例：

```python
>>> def gen_fn():
...     result = yield 1
...     print('result of yield: {}'.format(result))
...     result2 = yield 2
...     print('result of 2nd yield: {}'.format(result2))
...     return 'done'
```

要从另一个生成器调用此生成器，使用 `yield from` 进行委托：

```python
>>> # Generator function:
>>> def caller_fn():
...     gen = gen_fn()
...     rv = yield from gen
...     print('return value of yield-from: {}'
...           .format(rv))
...
>>> # Make a generator from the
>>> # generator function.
>>> caller = caller_fn()
```

调用方生成器的作用就好像它是 gen：

```python
>>> caller.send(None)
1
>>> caller.gi_frame.f_lasti
15
>>> caller.send('hello')
result of yield: hello
2
>>> caller.gi_frame.f_lasti  # Hasn't advanced.
15
>>> caller.send('goodbye')
result of 2nd yield: goodbye
return value of yield-from: done
Traceback (most recent call last):
  File "<input>", line 1, in <module>
StopIteration
```
`caller` yields from `gen`，但其指令指针（instruction pointer）不会向前移动，一直指在 15。即使内部生成器 gen 从一个 yield 语句前进到下一个。从 `caller` 的外部调用者的角度来看，我们无法判断它所产生的值是来自 `caller` 还是来自其委托的生成器。从 `gen` 内部，我们无法判断值是否从 `caller` 或外部传入。`yield from` 是一个无摩擦的通道，通过这个通道，在 `gen` 完成之前，值就会流入和流出。
coroutine 可以通过 `yield from` 将工作委派给下属，并得到工作成果。请注意，在上面，`caller` 打印了 "return value of yield-from: done"。当 `gen` 完成后，它的返回值就变成了在 `caller` 中 `yield from` 语句的值：

```python
rv = yield from gen
```



